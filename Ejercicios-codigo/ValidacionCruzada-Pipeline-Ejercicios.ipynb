{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalvarezme/AprendizajeMaquina/blob/main/3_Intro_Sklearn_Regresion/2_ValidacionCruzada_Regresion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpUmgSg5-Hpc"
      },
      "source": [
        "# Ejemplo completo para un problema de regresión usando sci-kitlearn\n",
        "\n",
        "El siguiente ejemplo presenta las etapas básicas de un proyecto de analítica de datos en una tarea de regresión, orientadas a:\n",
        "\n",
        "- Preproceso de atributos con campos vacios y tipo texto.\n",
        "- Entrenamiento y selección de un modelo de regresión bajo una estrategia de validación cruzada.\n",
        "- La utilización de diccionarios para la sintonización de hiperparámetros.\n",
        "-Se ilustra también la creación de clases (objetos) propios compatibles con la clase pipeline de sci-kitlearn.\n",
        "\n",
        "**Base de datos utilizada**: [FIFA 2019 - Kaggle](https://www.kaggle.com/datasets/devansodariya/football-fifa-2019-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZl3COcadq8T",
        "outputId": "4715e975-a771-4fd8-8e09-5f122df19bc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"wget\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n",
            "\"unzip\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " El volumen de la unidad C no tiene etiqueta.\n",
            " El n�mero de serie del volumen es: AA63-DCDC\n",
            "\n",
            " Directorio de c:\\GIT\\TAM-David-Ramirez\\Ejercicios-codigo\n",
            "\n",
            "20/09/2024  10:27 a. m.    <DIR>          .\n",
            "20/09/2024  10:27 a. m.    <DIR>          ..\n",
            "20/08/2024  09:15 p. m.             8,095 Bayes_David_Ramirez_Betancourth.ipynb\n",
            "20/08/2024  09:15 p. m.           415,514 Eigenfaces.ipynb\n",
            "20/08/2024  09:15 p. m.            58,249 Gaussiana_David_Ramirez_Betancourth.ipynb\n",
            "20/09/2024  10:27 a. m.           737,996 GPR-Ejercicio.ipynb\n",
            "09/09/2024  08:24 a. m.    <DIR>          image-segmentation\n",
            "20/09/2024  10:27 a. m.           296,200 KernelRidge-Ejercicio.ipynb\n",
            "20/09/2024  10:27 a. m.            36,547 ValidacionCruzada-Pipeline-Ejercicios.ipynb\n",
            "               6 archivos      1,552,601 bytes\n",
            "               3 dirs  381,749,059,584 bytes libres\n"
          ]
        }
      ],
      "source": [
        "#cargar datos desde drive acceso libre\n",
        "FILEID = \"15QxsIm_jMfHEy6Y--6o19IWgr66oiTRe\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O codigos.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip codigos.zip\n",
        "!dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljm0SDJm-4au"
      },
      "source": [
        "# Lectura de la base de datos con Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5hSsJ5r4exkC"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Main_Fifa/datos/data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain_Fifa/datos/data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#directorio de la base de datos en disco de colaboratory según archivo cargado\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m Xdata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#leer archivo csv\u001b[39;00m\n\u001b[0;32m     18\u001b[0m Xdata\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexcel_fifa_2019.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m col_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhoto\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlag\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClub Logo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaned From\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#variables a descartar - no contienen información relevante\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Main_Fifa/datos/data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#librerias para proceso\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "%matplotlib inline\n",
        "csv_path = 'Main_Fifa/datos/data.csv' #directorio de la base de datos en disco de colaboratory según archivo cargado\n",
        "\n",
        "Xdata = pd.read_csv(csv_path)#leer archivo csv\n",
        "Xdata.to_excel('excel_fifa_2019.xlsx', index=False)\n",
        "\n",
        "col_drop = ['Unnamed: 0', 'Name','ID','Photo','Flag',\n",
        "           'Club Logo','Loaned From'] #variables a descartar - no contienen información relevante\n",
        "Xdata.drop(columns = col_drop, inplace = True)\n",
        "Xdata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NvhrV6I_IhC"
      },
      "source": [
        "# Identificar tipos de variables, campos perdidos y preproceso básico a realizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6hrWmR5eFay"
      },
      "outputs": [],
      "source": [
        "Xdata.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnmfCo2RzTm8"
      },
      "source": [
        "## Se presentan atributos tipo int, float y texto, algunos de ellos con datos perdidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrC31OMozbX1"
      },
      "source": [
        "# Partición en conjunto de training y testing.\n",
        "\n",
        "## Tips básicos para escoger la estrategia de validación:\n",
        "\n",
        "- Para $N\\geq 5000$ se sugiere realizar entrenamiento hold-out, generando grupos de entrenamiento, validación y evaluación. Los conjuntos de entrenamiento y validación se pueden utilizar para realizar validación cruzada de 10 o 5 folds para determinar el mejor modelo y sus hiperparámetros.\n",
        "\n",
        "- No obstante, si la cantidad de datos es muy grande, generalmente se utiliza un único conjunto de entrenamiento y un único conjunto de validación, e.g., modelos de deep learning generalmente utilizan esta estrategia por la gran cantidad de datos disponible.\n",
        "\n",
        "**HoldOut con única partición de entrenamiento, validación y evaluación (training, validation, testing)**\n",
        "\n",
        "![HoldOut](https://github.com/amalvarezme/AprendizajeMaquina/blob/main/3_Intro_Sklearn_Regresion/HoldOut.png?raw=1)\n",
        "\n",
        "\n",
        "**HoldOut utilizando validación cruzada para definir varios conjuntos de entrenamiento y validación en la búsqueda del mejor modelo con evaluación final en un único conjunto de testing**\n",
        "\n",
        "![grid_search_cross_validation](https://github.com/amalvarezme/AprendizajeMaquina/blob/main/3_Intro_Sklearn_Regresion/crossvalidation.png?raw=1)\n",
        "\n",
        "- Se sugiere para $ 30 <N <5000$ realizar validación cruzada o validación cruzada anidad k folds (generalmente se trabaja con k=10 o k = 5), y reportar el desempeño promedio en el conjunto de test a lo largo de las rotaciones.\n",
        "\n",
        "- En algunos textos, dado que los datos se rotan y aparecen al menos una vez en el conjunto de training y una vez en el de testing, se habla solamente de conjuntos de entrenamiento y validación, porque el concepto de datos afuera (testing) se pierde.\n",
        "\n",
        "![kfolds](https://github.com/amalvarezme/AprendizajeMaquina/blob/main/3_Intro_Sklearn_Regresion/kfolds.png?raw=1)\n",
        "\n",
        "- La validación cruzada anidad es mucho más estricta a la hora de dejar un conjunto de test que no se utiliza para sintonizar los hiperparámetros (a diferencia del kfolds convencional que determina los mejores valores de hiperparámetros respecto al conjunto de validación=evaluación). Sin embargo, la validación anidad es más costosa computacionalmente.\n",
        "\n",
        "\n",
        "![nestedcv](https://github.com/amalvarezme/AprendizajeMaquina/blob/main/3_Intro_Sklearn_Regresion/nestedcv.png?raw=1)\n",
        "\n",
        "- Para $N \\leq 30$ se sugiere trabajar con validación cruzada fijando el número de particiones igual al número de datos ($k=N$). En este caso, se tiene un conjunto de entrenamiento de $N-1$ y una única muestra de test que se rota $N$ veces (leave-one-out).\n",
        "\n",
        "\n",
        "\n",
        "**Nota**: Para el ejemplo ilustrado se cuenta con una buena cantidad de datos 18.000 aprox., por lo que se realizará una validación hold-out, con 70% de datos para entrenamiento y 30% para evaluación. Sobre el conjunto de entrenamiento se realizará una validación cruzada de 5 folds (k=5) para determinar el mejor modelo e hiperparámetros.\n",
        "\n",
        "Se define el atributo Release Clause como salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM_si20zfVQz"
      },
      "outputs": [],
      "source": [
        "# Partición entrenamiento y evaluación\n",
        "# Tamaño Xtrain 70%, Tamaño Xtest 30%\n",
        "\n",
        "Xtrain, Xtest = train_test_split(Xdata,test_size=0.3)\n",
        "col_sal = \"Release Clause\"\n",
        "ytrain = Xtrain[col_sal]\n",
        "ytest = Xtest[col_sal]\n",
        "Xtrain.drop(columns=col_sal,inplace=True)\n",
        "Xtest.drop(columns=col_sal,inplace=True)\n",
        "\n",
        "\n",
        "print('Xtrain=', Xtrain.shape, 'Xtest=',Xtest.shape,'ytrain=', ytrain.shape, 'ytest=', ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRAlhmGH5hMH"
      },
      "source": [
        "**Nota**: Recuerde que puede usar la partición estratificada para preservar la distribución de la salida de interés en los folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ogS5bI3-Wkf"
      },
      "source": [
        "# Preproceso: completar datos y codificar datos tipo texto\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHjxl2-2iNzX"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")#se utiliza estimador de moda\n",
        "ytrain = imputer.fit_transform(pd.DataFrame(ytrain)).reshape(-1)#ajuste sobre la salida\n",
        "ytest = imputer.transform(pd.DataFrame(ytest)).reshape(-1)#evaluar sobre datos de test\n",
        "\n",
        "print('ytrain=', ytrain.shape, 'ytest=', ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA68DNCJ-vEg"
      },
      "source": [
        "**Nota**: Dado que la variable de salida esta en formato moneda se debe codificar meidante función particular (custom). También, se creará una función para codificar las variables tipo evolución características del jugador.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ErWPFYnbcfa"
      },
      "outputs": [],
      "source": [
        "ytrain[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LgKYWa0l9xc"
      },
      "outputs": [],
      "source": [
        " #codificar variables moneda\n",
        "def code_euro(y):\n",
        "    yc = np.zeros(y.shape[0])\n",
        "    for i in range(y.shape[0]):\n",
        "        if y[i][-1]=='M': yc[i] = float(y[i][1:-1])*10**6 #buscar M y reemplazar 10^6\n",
        "        elif y[i][-1]=='K': yc[i] = float(y[i][1:-1])*10**3 # buscar K y reemplazar por 10^3\n",
        "        else: yc[i] = float(y[i][1:])\n",
        "    return yc\n",
        "\n",
        "#codificar estadísticas\n",
        "def code_stats(y):\n",
        "    yc = np.zeros(y.shape[0])\n",
        "    for i in range(y.shape[0]):\n",
        "        if y.iloc[i].find(\"+\") > -1: # encontrar signo + en str y casteo a flotante\n",
        "            yc[i] = float(y.iloc[i][:y.iloc[i].find(\"+\")])+float(y.iloc[i][y.iloc[i].find(\"+\")+1:])\n",
        "        else: yc[i] = float(y.iloc[i])\n",
        "    return yc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MU_7r93ndmv"
      },
      "outputs": [],
      "source": [
        "#codificar salida\n",
        "ytrain_num = code_euro(ytrain)\n",
        "ytest_num = code_euro(ytest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPODsXh8ntmQ"
      },
      "outputs": [],
      "source": [
        "ytrain_num[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8apkq-qxAX12"
      },
      "source": [
        "## Se definen las variables para codificar utilizando las funciones establecidas de moneda, estadísticas y las tipo texto con OrdinalEncoder y OneHotEncoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i4yEtRcoC1F"
      },
      "outputs": [],
      "source": [
        "#definir columnas tipo string  para codificar moneda, estadistica fifa y categoricas\n",
        "col_euro = ['Value','Wage'] #variables tipo moneda\n",
        "col_stats = ['LS','ST','RS','LW','LF','CF','RF','RW','LAM',\n",
        "             'CAM','RAM','LM','LCM','CM','RCM','RM','LWB',\n",
        "             'LDM','CDM','RDM','RWB','LB','LCB','CB','RCB','RB']#variables tipo mejora jugadores\n",
        "\n",
        "cat = ['Nationality','Club','Body Type','Position','Preferred Foot',\n",
        "        'Work Rate','Real Face','Contract Valid Until']#variables a codificar OrdinalEncoder y OneHotEncoder\n",
        "items = []\n",
        "for i in cat:\n",
        "    items += [list(Xdata[i].value_counts().index)]\n",
        "cat_usr = dict(zip(cat, items))#se crean diccionarios con las variables y sus posibles valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhk8i2RTBl7e"
      },
      "outputs": [],
      "source": [
        "Xdata['Nationality'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sAWcEWyAw89"
      },
      "outputs": [],
      "source": [
        "cat_usr.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDnsPyAddXed"
      },
      "outputs": [],
      "source": [
        "cat_usr['Body Type']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AzzTDXFYLyU"
      },
      "source": [
        "# Se crea clase propia (custom) para ejecutar el preproceso que sea compatible con scikitlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L511SbLFq4xM"
      },
      "outputs": [],
      "source": [
        "#%% transformer custom\n",
        "from sklearn.base import BaseEstimator, TransformerMixin #objetos básicos para herencia de clase\n",
        "\n",
        "class mypre_fifa(BaseEstimator,TransformerMixin):\n",
        "    #inicializacion de clase y varaibles\n",
        "    def __init__(self, col_euro,col_stats, cat_usr): #constructor clase\n",
        "        self.col_euro = col_euro #lista atributos tipo moneda\n",
        "        self.col_stats = col_stats #lista atributos tipo estadistica\n",
        "        self.cat_usr = cat_usr #lista de atributos categoricos\n",
        "\n",
        "    def fit(self,X, *_):#función de ajuste -> *_ para indicar que puede recibir más entradas en el pipeline\n",
        "        Xi = X.copy() #copiar dataset para no reemplazar original\n",
        "        self.imputer_num = SimpleImputer(strategy=\"most_frequent\") #crear imputador tipo moda\n",
        "        self.a = Xi.columns[np.sum(Xi.isna())> 0] #encontrar columnas con datos faltantes\n",
        "        self.imputer_num.fit(Xi[self.a]) # ajustar imputador\n",
        "        Xi[self.a] = self.imputer_num.transform(Xi[self.a]) #evaluar datos con el imputador\n",
        "\n",
        "        for i in self.col_euro: #codificar tipo moneda\n",
        "            Xi[i] = code_euro(np.array(Xi[i]))\n",
        "\n",
        "        for i in self.col_stats: #codificar datos estadisticos\n",
        "            Xi[i] = code_stats(Xi[i])\n",
        "\n",
        "        #height, wieght corregir formato\n",
        "        Xi['Height'].replace(regex=[\"'\"], value='.',inplace=True) #reemplaza unidad ' con .\n",
        "        for i in Xi.index:\n",
        "            Xi.loc[i,'Weight'] = float(Xi.loc[i,'Weight'][:-3])#elimnar unidades\n",
        "            Xi.loc[i,'Height'] = float(Xi.loc[i,'Height'])#eliminar unidades\n",
        "\n",
        "        Xi['Height'] = Xi['Height'].astype('float64')\n",
        "        Xi['Weight'] = Xi['Weight'].astype('float64')#asegurar formato flotante\n",
        "\n",
        "        Xi['Joined'] = Xi['Joined'].replace(regex=\"/\",value=\"\")#codificar fecha\n",
        "        Xi['Joined'] = Xi['Joined'].astype('float64')\n",
        "\n",
        "        cat = [] #codificar variables categoricas con ordinal encoder\n",
        "        for i in self.cat_usr.keys():\n",
        "            cat = cat + [[*self.cat_usr.get(i)]]\n",
        "        self.col_cat_usr = OrdinalEncoder(categories=cat) # OneHotEncoder(categories=cat,sparse=False)\n",
        "        Xi[[*self.cat_usr.keys()]] =self.col_cat_usr.fit_transform(Xi[[*self.cat_usr.keys()]])\n",
        "        #si utilizar OneHotEncoder debe concatenar las columnas resultantes -> ver cuaderno guia lado a lado\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):#funcion transformador-> pensando en datos nuevos\n",
        "        Xi = X.copy()\n",
        "        Xi[self.a] = self.imputer_num.transform(Xi[self.a])\n",
        "\n",
        "        for i in self.col_euro:\n",
        "            Xi[i] = code_euro(np.array(Xi[i]))\n",
        "        for i in self.col_stats:\n",
        "            Xi[i] = code_stats(Xi[i])\n",
        "\n",
        "                #height, wieght\n",
        "        Xi['Height'].replace(regex=[\"'\"], value='.',inplace=True)\n",
        "        for i in Xi.index:\n",
        "            Xi.loc[i,'Weight'] = float(Xi.loc[i,'Weight'][:-3])\n",
        "            Xi.loc[i,'Height'] = float(Xi.loc[i,'Height'])\n",
        "\n",
        "        Xi['Height'] = Xi['Height'].astype('float64')\n",
        "        Xi['Weight'] = Xi['Weight'].astype('float64')\n",
        "\n",
        "        Xi['Joined'] = Xi['Joined'].replace(regex=\"/\",value=\"\")\n",
        "        Xi['Joined'] = Xi['Joined'].astype('float64')\n",
        "\n",
        "        Xi[[*self.cat_usr.keys()]] =self.col_cat_usr.transform(Xi[[*self.cat_usr.keys()]])\n",
        "        return Xi\n",
        "\n",
        "    def fit_transform(self,X,*_):#ajustar y transformar en cascada\n",
        "        self.fit(X)\n",
        "        return self.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6vm9wF_q_8E"
      },
      "outputs": [],
      "source": [
        "mypre = mypre_fifa(col_euro=col_euro,col_stats=col_stats,cat_usr = cat_usr)\n",
        "Xtrain_pre = mypre.fit_transform(Xtrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmDDvZweDU-B"
      },
      "source": [
        "**Nota**: Recuerde que ninguna etapa del proceso puede utilizar los datos de test para sintonizar parámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scFQR2V6mXPA"
      },
      "outputs": [],
      "source": [
        "Xtest_pre = mypre.transform(Xtest) #no se puede fit solo transform-> parámetros ajustados sobre Xtrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir2sCB3lfQsc"
      },
      "outputs": [],
      "source": [
        "Xtrain_pre.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYANuI5WDgeH"
      },
      "source": [
        "Las variables fueron codificadas $X_{train}\\in\\mathbb{R}^{N \\times P}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ZSblP8Do1d"
      },
      "source": [
        "# Se realiza análisis exploratorio básico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izTxZ1_vgfgZ"
      },
      "outputs": [],
      "source": [
        "corr_matrix = Xtrain_pre.corr()#matriz de correlación\n",
        "corr_matrix.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xg6WYtMD8HF"
      },
      "source": [
        "# Ejercicio 1:\n",
        "\n",
        "1. Qué puede discutir sobre las correlaciones cálculadas?\n",
        "\n",
        "2. Cuáles son los atributos que presentan una relación lineal más fuerte con la variable de salida?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekpm0ZfxqMbG"
      },
      "source": [
        "# Ejercicio 2:\n",
        "\n",
        "Sobre los datos preprocesados de entrenamiento presente las gráficas de histograma, dispersión (scatter matrix) y diagramas de caja para la salida y al menos tres atirbutos de entrada, teniendo en cuenta: i) datos preprocesados, ii) datos preprocesados más StandardScaler, iii) datos preprocesados más MinMaxScaler.\n",
        "\n",
        "**Nota**: se presentan algunas líneas de código de ayuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_iXCwJeFfid"
      },
      "outputs": [],
      "source": [
        "Xtrain_pre.columns #atributos disponibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKjDz6WqFWto"
      },
      "outputs": [],
      "source": [
        "Xm =pd.DataFrame(Xtrain_pre[['Age','Overall','Potential']],columns=['Age','Overall','Potential'])#definir pandas con atributos seleccionados\n",
        "Xm['Output'] = ytrain_num #agregar salida\n",
        "\n",
        "Xtrain_pre2 = Xtrain_pre.copy()\n",
        "Xtrain_pre2['output'] = ytrain_num\n",
        "corr_matrix2 = Xtrain_pre2.corr()#matriz de correlación\n",
        "corr_matrix2.style.background_gradient(cmap='coolwarm')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJiT2G9dwsZh"
      },
      "outputs": [],
      "source": [
        "abs(corr_matrix2[\"output\"]).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC-CasFdF3rp"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "Xm.boxplot()#grafica de caja\n",
        "plt.show()\n",
        "scatter_matrix(Xm, figsize=(12, 8)) #scatter con histogramas\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bKPr_yuxlXU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#scaler = StandardScaler()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "Xm_pre_sca = pd.DataFrame(scaler.fit_transform(Xm),columns=Xm.columns)\n",
        "Xm_pre_sca.boxplot()#grafica de caja\n",
        "plt.show()\n",
        "scatter_matrix(Xm_pre_sca, figsize=(12, 8)) #scatter con histogramas\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N51EkyMNL-K"
      },
      "source": [
        "# Se define pipeline para entrenar un modelo de regresión\n",
        "\n",
        "**Nota:** Se sugiere utilizar un preproceso de normalización tipo StandardScaler o MinMaxScaler para evitar problemas de escala a la hora de identificar los patrones de interés.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vuC7dShGaGe"
      },
      "source": [
        "A continuación se genera pipeline completo inlcuyendo preproceso custom, standardscaler y modelo de regresión lineal por mínimos cuadrados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzSaOdfipj0f"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "steps = [('preproceso',mypre_fifa(col_euro=col_euro,col_stats=col_stats,cat_usr = cat_usr)),\n",
        "         ('scaler', StandardScaler()),\n",
        "         ('regresion',LinearRegression())\n",
        "         ]\n",
        "metodo_full = Pipeline(steps=steps)\n",
        "metodo_full.fit(Xtrain,ytrain_num) #entrena todos los pasos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEfIsVM4r9Vt"
      },
      "outputs": [],
      "source": [
        "ytest_e = metodo_full.predict(Xtest) #se evalua el desempeño en el conjunto de test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNNoAX_Gt16"
      },
      "source": [
        "Se grafican salida original y las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY17CQi0tFcY"
      },
      "outputs": [],
      "source": [
        "plt.plot(ytest_num,label='original')\n",
        "plt.plot(ytest_e,'r',label='estimado')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxxjCksUyiOt"
      },
      "outputs": [],
      "source": [
        "plt.scatter(ytest_num,ytest_e)\n",
        "plt.xlabel('ytest')\n",
        "plt.ylabel('ytest_e')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLAvnq4mHFRl"
      },
      "source": [
        "# Ejercicio 3\n",
        "\n",
        "1. Utilizando el método steps del pipeline, realice una gráfica de las cargas (pesos) del modelo lineal entrenado. Qué puede discutir al respecto?\n",
        "\n",
        "2. Consulte y presente el modelo y problema de optimización de los siguientes regresores:\n",
        "\n",
        " - [LinearRegresor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
        " - [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
        " - [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
        " - [KernelRidge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)\n",
        " - [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
        " - [BayesianRidge](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression)\n",
        " - [Gaussian Process Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)\n",
        "\n",
        "3. Utilizando un esquema de validación cruzada de 5 folds sobre el conjunto de entrenamiento preprocesado, compare el rendimiento en el conjunto de test de los regresores del punto 2. fijando el score del [gridsearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) en términos del error absoluto medio y el error cuádratico medio. Justifique los hiperparámetros a buscar y la rejilla de valores escogida para cada algoritmo según los modelos estudiados en clase y las respuestas del punto 2. Para el caso del kernelRidge y GPR se sugiere trabajar sobre un conjunto de train de 5000 puntos para evitar problemas de memoría en los cálculos de la matriz kernel (presente los resultados para kernel ridge utilizando una función rbf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWOo3Vsqlgfy"
      },
      "outputs": [],
      "source": [
        "metodo_full.steps[2][1].coef_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKJqo8XdIkHu"
      },
      "outputs": [],
      "source": [
        "#Ayuda punto 1:\n",
        "#el pipeline contiene el método steps codificando los elementos del proceso.\n",
        "#En este caso el regresor esta en la posición 2 y en la 1 de la tupla arrojada\n",
        "#Revisando la documentación de LinearRegression, el atributo coef_ contiene las cargas del modelo\n",
        "plt.stem(abs(metodo_full.steps[2][1].coef_))\n",
        "plt.xlabel('atributo')\n",
        "plt.ylabel('$w_j$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3qeSCq_Ryuj"
      },
      "outputs": [],
      "source": [
        "#Ayuda punto 2 y 3\n",
        "#Dado que el preproceso no cambia, se sugiera generar el pipeline sin mypre_fifa para ahorrar tiempo\n",
        "steps = [('scaler', StandardScaler()), #ajustar según ejercicio\n",
        "         ('regresion',LinearRegression()) #ajustar según ejercicio\n",
        "         ]\n",
        "metodo_gs = Pipeline(steps=steps) #pipeline a utilizar en el gridsearch\n",
        "#Recuerde realizar el fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "#Xtrain_pre ytrain_num\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Dj7iTZcoXX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('regresion', Lasso(alpha=1.0))]  # Ajusta alpha según sea necesario\n",
        "\n",
        "metodo_gs = Pipeline(steps=steps)\n",
        "\n",
        "# Fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEV4JbCndBZY"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('regresion', ElasticNet(alpha=1.0, l1_ratio=0.5))]  # Ajusta alpha y l1_ratio según sea necesario\n",
        "\n",
        "metodo_gs = Pipeline(steps=steps)\n",
        "\n",
        "# Fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF36touLdbAn"
      },
      "outputs": [],
      "source": [
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('regresion', KernelRidge(alpha=1.0, kernel='linear'))]  # Ajusta alpha y el kernel según sea necesario\n",
        "\n",
        "metodo_gs = Pipeline(steps=steps)\n",
        "\n",
        "# Fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU8Inf1Rd-3F"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('regresion', SGDRegressor(max_iter=1000, tol=1e-3, alpha=0.0001))]  # Ajusta los parámetros según sea necesario\n",
        "\n",
        "metodo_gs = Pipeline(steps=steps)\n",
        "\n",
        "# Fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl_hj3mod_rh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('regresion', BayesianRidge())]  # Ajusta los parámetros según sea necesario\n",
        "\n",
        "metodo_gs = Pipeline(steps=steps)\n",
        "\n",
        "# Fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1xz0jpueGFo"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "\n",
        "# Definir un kernel\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('regresion', GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10))]\n",
        "\n",
        "metodo_gs = Pipeline(steps=steps)\n",
        "\n",
        "# Fit del gridsearch sobre las matrices de entrada y vector de salida preprocesados\n",
        "metodo_gs.fit(Xtrain_pre, ytrain_num)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
